#!/usr/local/bin/python3

# Copyright 2012-2013 Erik-Jan van Baaren (erikjan@gmail.com)
# This tool is released as part of the python package ESClient which can be found on PyPI.org
# or on github at https://github.com/eriky/ESClient

import esclient
import json
import argparse
import sys
from datetime import datetime as dt
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
from urllib.parse import urlparse,urlsplit

parser = argparse.ArgumentParser(description="Dump one or more ElasticSearch" +
" indexes to stdout. This tool will dump all the _source fields. If you chose"+
" not to store the _source field, you can not make backups of your index(es)"+
" with this tool.")

parser.add_argument('--url', '-u', required=True, help="The full URL to the ElasticSearch server, including port")
parser.add_argument('--file', '-f', required=False, help="The output file to dump to. By default esdump will dump to stdout.")
parser.add_argument('--mappingfile', '-m', required=False, help="The mapping file to dump to. By default esdump will not save mapping.")
parser.add_argument('--indexes', '-i', nargs='+', help="One or more index names to dump, may also be aliases. If none specified, ALL indexes are dumped.")
parser.add_argument('--gzip', '-z', action="store_true", help="Use gzip to compress the output")
parser.add_argument('--bzip2', '-b', action="store_true", help="Use bzip2 to compress the output")
parser.add_argument('--stored-fields', '-s', nargs='+', help="A list of fields that you want to include in the backup (_source, _id, _parent and _routing are included automatically")
parser.add_argument('--count', '-c', action="store_true", help="Only print the document count")

arguments = parser.parse_args()

if not arguments.indexes:
    indexes = ['_all']
else:
    indexes = arguments.indexes

if arguments.bzip2 and arguments.gzip:
    sys.stderr.write("Invalid combination of options: I can write either bzip2 or gzip, not both.\n")
    sys.exit(1)
 
es = esclient.ESClient(arguments.url)

# TODO: check cluster state before continuing
def fail_exit(msg):
    sys.stderr.write(msg + "\n")
    sys.exit(1)

# Open a file to write to, based on the arguments given 
if arguments.bzip2 and arguments.file:
    import bz2
elif arguments.gzip and arguments.file:
    import gzip
elif arguments.file:
else:
    if arguments.bzip2 or arguments.gzip:
        fail_exit("This tool will not write compressed output to stdout. You can however pipe the output through gzip or bzip2 to compress the data.")
    else:    
        # use stdout as a file
        f = sys.stdout

if (arguments.count):
    if len(indexes) != 1:
        fail_exit("You cannot use zero or multiple indexes when count is given.")

    count_result = es.count(None, None, arguments.indexes)
    if not 'count' in count_result:
        fail_exit("You cannot use zero or multiple indexes when count is given.")

    #print str(count_result['count'])
    sys.exit(0)

if (arguments.mappingfile):
    if len(indexes) != 1:
        fail_exit("You cannot use zero or multiple indexes when using mapping file.")

    mappingfile = open(arguments.mappingfile, "w")
    mapping = es.get_mapping(indexes)

    if 'error' in mapping:
        raise Exception("Mapping file contains error: " + str(mapping['error']))

    mappingfile.write(json.dumps(mapping))
    mappingfile.close()


# Get the fields that the user wants to backup in addition to the system fields as listed below
fields = set(["_parent", "_routing", "_source"])
if arguments.stored_fields:
    for field in arguments.stored_fields:
        fields.add(field)
fields = list(fields)

query_body = { "query": { "match_all": {} }, "fields": fields }
scroll_id = es.scan(query_body = query_body, indexes = indexes)

indexes = set()

querys = []
while True:
    scrollres = es.scroll(scroll_id)
    # get next scroll_id
    scroll_id = scrollres["_scroll_id"]
    
    hits = scrollres["hits"]["hits"]

    num_results = 0

    def data_check(x):
        if x == '-' or x == '':
            return None

        return x

    def split_parameters(params):
        return  dict(item.split("=") for item in params.split("&"))

    def split_path(path,prefix='_path'):
        paths = list(filter(data_check , path.split('/')))
        prefixes = map(lambda x: prefix + str(x+1) ,range(len(paths)))
        return dict(zip(prefixes , paths))

    def datetime_format(timestamp):
        return dt.strptime(timestamp, '%Y-%m-%dT%H:%M:%S+09:00')


    for hit in scrollres["hits"]["hits"]:
        # Delete this field, since it is useless for restore purposes
        del(hit["_score"])
        if hit["_index"] not in indexes:
            indexes.add(hit["_index"])

        source = hit['_source']

        # ELBヘルスチェックは集計しない
        if 'ELB-HealthChecker' in source.get('agent'):
            continue

        path = urlsplit(source['path'])

        if path.query:            
            record = split_parameters(path.query)                
        else:
            record = {}

	# 必要な情報を追加
        record['_timestamp'] = datetime_format(source.get('@timestamp'))
        record['_ua'] = data_check(source.get('agent'))
        record.update(split_path(path.path))
        # keyをインデックスとしてPush
        querys.append(
            Series(record))
            
        #print(querys)
        #sys.exit()
        num_results += 1

    # See if we reached the end of the data
    if num_results == 0:
        break

# TODO
# write_mappings(indexes)

if querys:
   log = DataFrame(querys) 
   log.to_csv(arguments.file)


