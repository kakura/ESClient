#!/usr/local/bin/python3

# Copyright 2012-2013 Erik-Jan van Baaren (erikjan@gmail.com)
# This tool is released as part of the python package ESClient which can be found on PyPI.org
# or on github at https://github.com/eriky/ESClient

import esclient
import json
import argparse
import sys
from datetime import datetime as dt
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
from urllib.parse import urlparse,urlsplit

parser = argparse.ArgumentParser(description="Dump one or more ElasticSearch" +
" indexes to stdout. This tool will dump all the _source fields. If you chose"+
" not to store the _source field, you can not make backups of your index(es)"+
" with this tool.")

parser.add_argument('--url', '-u', required=True, help="The full URL to the ElasticSearch server, including port")
parser.add_argument('--file', '-f', required=False, help="The output file to dump to. By default esdump will dump to stdout.")
parser.add_argument('--mappingfile', '-m', required=False, help="The mapping file to dump to. By default esdump will not save mapping.")
parser.add_argument('--indexes', '-i', nargs='+', help="One or more index names to dump, may also be aliases. If none specified, ALL indexes are dumped.")
parser.add_argument('--stored-fields', '-s', nargs='+', help="A list of fields that you want to include in the backup (_source, _id, _parent and _routing are included automatically")
parser.add_argument('--count', '-c', action="store_true", help="Only print the document count")

parser.add_argument('--ignore_path', '-g',required=False, help="ignore file")
parser.add_argument('--select_path', '-p',required=False, help="select file")

arguments = parser.parse_args()

if not arguments.indexes:
    indexes = ['_all']
else:
    indexes = arguments.indexes

es = esclient.ESClient(arguments.url)

# TODO: check cluster state before continuing
def fail_exit(msg):
    sys.stderr.write(msg + "\n")
    sys.exit(1)


if (arguments.count):
    if len(indexes) != 1:
        fail_exit("You cannot use zero or multiple indexes when count is given.")

    count_result = es.count(None, None, arguments.indexes)
    if not 'count' in count_result:
        fail_exit("You cannot use zero or multiple indexes when count is given.")

    #print str(count_result['count'])
    sys.exit(0)

if (arguments.mappingfile):
    if len(indexes) != 1:
        fail_exit("You cannot use zero or multiple indexes when using mapping file.")

    mappingfile = open(arguments.mappingfile, "w")
    mapping = es.get_mapping(indexes)

    if 'error' in mapping:
        raise Exception("Mapping file contains error: " + str(mapping['error']))

    mappingfile.write(json.dumps(mapping))
    mappingfile.close()


# Get the fields that the user wants to backup in addition to the system fields as listed below
fields = set(["_parent", "_routing", "_source"])
if arguments.stored_fields:
    for field in arguments.stored_fields:
        fields.add(field)
fields = list(fields)

query_body = { "query": { "match_all": {} }, "fields": fields }

if arguments.select_path:
    select_paths = arguments.select_path.split(',')
    query = ' OR '.join(select_paths)
    query_body = {"query":
                  {"filtered":{
                      "query": {
                          "query_string":{
                                "query": query
                                }
                          }
                      }
                  },"fields": fields}
    
scroll_id = es.scan(query_body = query_body, indexes = indexes)

indexes = set()

querys = []

def data_check(x):
    if x == '-' or x == '':
        return None

    return x

def split_parameters(params):
    table = []
    for items in params.split("&"):
        if items:
            item = items.split("=")
            if len(item) <= 2:
                table.append(item)

    return  dict(table)

def split_path(path,prefix='_path'):
    paths = list(filter(data_check , path.split('/')))
    prefixes = map(lambda x: prefix + str(x+1) ,range(len(paths)))
    return dict(zip(prefixes , paths))

def datetime_format(timestamp):
    return dt.strptime(timestamp, '%Y-%m-%dT%H:%M:%S+09:00')

def search_path(path,paths):
    for p in paths:
        if p in path:
            return True

    return False

def string2int16(x):
    if x != x:
        return x
    else:
        return int(x,16)

def dms2deg(coord):
    return (int(coord) / 60 / 60 / 1000)

def tokyo2wgs(lat, lng):
    lat = dms2deg(lat)
    lng = dms2deg(lng)
    return {
        "lat": (lat - 0.00010695 * lat + 0.000017464 * lng + 0.0046017),
        "lng": (lng - 0.000046038 * lat - 0.000083043 * lng + 0.010041)
    }

list_16 = ['ost','odt','gcf','clt']
list_coord = {'s':{'lng' :'stx','lat':'sty'},
              'e':{'lng' :'edx','lat':'edy'},
              'm1':{'lng':'mx1','lat':'my1'},
              'm2':{'lng':'mx2','lat':'my2'},
              'm3':{'lng':'mx3','lat':'my3'}}
    
ignore_flg = False
if arguments.ignore_path:
    ignore_paths = arguments.ignore_path.split(',')
    ignore_flg = True

while True:
    # records
    scrollres = es.scroll(scroll_id)
    # get next scroll_id
    scroll_id = scrollres["_scroll_id"]
    
    hits = scrollres["hits"]["hits"]

    num_results = 0
                
    for hit in scrollres["hits"]["hits"]:
        record = None
        # Delete this field, since it is useless for restore purposes
        if hit["_index"] not in indexes:
            indexes.add(hit["_index"])

        num_results += 1
        source = hit['_source']

        # ignore_list
        if ignore_flg:
            if search_path(source['path'], ignore_paths) == True:
                continue
            
        if 'ELB-HealthChecker' in source.get('agent'):
            continue

        path = urlsplit(source['path'])
        if path.query != '':
            record = split_parameters(path.query)                
        else:
            record = {}

        record['_timestamp'] = datetime_format(source.get('@timestamp'))
        record['_ua'] = data_check(source.get('agent'))
        record['_size'] = data_check(source.get('size'))
        record.update(split_path(path.path))

        for i in list_16:
            if (i in record) and record[i]:
                record[i] = string2int16(record[i])

        for k,v in list_coord.items():
            if (v['lat'] in record) and (v['lng'] in record):
                latlng = tokyo2wgs(record[v['lat']],record[v['lng']])
                record[v['lat']] = latlng['lat']
                record[v['lng']] = latlng['lng']
            
        querys.append(
            Series(record))
            
        #print(querys)
        #sys.exit()

    # See if we reached the end of the data
    if num_results == 0:
        break

# TODO
# write_mappings(indexes)

if querys:
   log = DataFrame(querys) 
   log.to_csv(arguments.file)


